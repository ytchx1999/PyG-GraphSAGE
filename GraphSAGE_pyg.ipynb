{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cora()\n"
     ]
    }
   ],
   "source": [
    "dataset_cora = Planetoid(root='./cora/', name='Cora')\n",
    "# dataset = Planetoid(root='./citeseer',name='Citeseer')\n",
    "# dataset = Planetoid(root='./pubmed/',name='Pubmed')\n",
    "print(dataset_cora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = SAGEConv(dataset_cora.num_node_features, 16)\n",
    "        self.conv2 = SAGEConv(16, dataset_cora.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): SAGEConv(1433, 16)\n",
      "  (conv2): SAGEConv(16, 7)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Data(edge_index=[2, 10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model.to(device)\n",
    "data = dataset_cora[0].to(device)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 train_loss: 1.9452 train_acc: 0.1571\n",
      "Epoch 001 train_loss: 1.9229 train_acc: 0.4214\n",
      "Epoch 002 train_loss: 1.8821 train_acc: 0.5643\n",
      "Epoch 003 train_loss: 1.8269 train_acc: 0.6071\n",
      "Epoch 004 train_loss: 1.7725 train_acc: 0.5714\n",
      "Epoch 005 train_loss: 1.7015 train_acc: 0.6857\n",
      "Epoch 006 train_loss: 1.6359 train_acc: 0.7786\n",
      "Epoch 007 train_loss: 1.5815 train_acc: 0.8071\n",
      "Epoch 008 train_loss: 1.5096 train_acc: 0.8786\n",
      "Epoch 009 train_loss: 1.4612 train_acc: 0.8929\n",
      "Epoch 010 train_loss: 1.4121 train_acc: 0.9286\n",
      "Epoch 011 train_loss: 1.3761 train_acc: 0.9429\n",
      "Epoch 012 train_loss: 1.3267 train_acc: 0.9643\n",
      "Epoch 013 train_loss: 1.3006 train_acc: 0.9786\n",
      "Epoch 014 train_loss: 1.2802 train_acc: 0.9857\n",
      "Epoch 015 train_loss: 1.2341 train_acc: 0.9857\n",
      "Epoch 016 train_loss: 1.2457 train_acc: 0.9929\n",
      "Epoch 017 train_loss: 1.2292 train_acc: 0.9929\n",
      "Epoch 018 train_loss: 1.2242 train_acc: 0.9714\n",
      "Epoch 019 train_loss: 1.2120 train_acc: 0.9857\n",
      "Epoch 020 train_loss: 1.2384 train_acc: 0.9500\n",
      "Epoch 021 train_loss: 1.2013 train_acc: 0.9857\n",
      "Epoch 022 train_loss: 1.1997 train_acc: 0.9929\n",
      "Epoch 023 train_loss: 1.2156 train_acc: 0.9714\n",
      "Epoch 024 train_loss: 1.2055 train_acc: 0.9857\n",
      "Epoch 025 train_loss: 1.1951 train_acc: 0.9929\n",
      "Epoch 026 train_loss: 1.1920 train_acc: 0.9857\n",
      "Epoch 027 train_loss: 1.2038 train_acc: 0.9786\n",
      "Epoch 028 train_loss: 1.1979 train_acc: 0.9929\n",
      "Epoch 029 train_loss: 1.1873 train_acc: 1.0000\n",
      "Epoch 030 train_loss: 1.1894 train_acc: 0.9857\n",
      "Epoch 031 train_loss: 1.1781 train_acc: 1.0000\n",
      "Epoch 032 train_loss: 1.1899 train_acc: 0.9929\n",
      "Epoch 033 train_loss: 1.1737 train_acc: 1.0000\n",
      "Epoch 034 train_loss: 1.1870 train_acc: 1.0000\n",
      "Epoch 035 train_loss: 1.1798 train_acc: 1.0000\n",
      "Epoch 036 train_loss: 1.1772 train_acc: 1.0000\n",
      "Epoch 037 train_loss: 1.1777 train_acc: 1.0000\n",
      "Epoch 038 train_loss: 1.1803 train_acc: 1.0000\n",
      "Epoch 039 train_loss: 1.1801 train_acc: 1.0000\n",
      "Epoch 040 train_loss: 1.1819 train_acc: 0.9929\n",
      "Epoch 041 train_loss: 1.1805 train_acc: 1.0000\n",
      "Epoch 042 train_loss: 1.1799 train_acc: 0.9929\n",
      "Epoch 043 train_loss: 1.1805 train_acc: 1.0000\n",
      "Epoch 044 train_loss: 1.1800 train_acc: 1.0000\n",
      "Epoch 045 train_loss: 1.1832 train_acc: 0.9929\n",
      "Epoch 046 train_loss: 1.1832 train_acc: 0.9929\n",
      "Epoch 047 train_loss: 1.1850 train_acc: 0.9929\n",
      "Epoch 048 train_loss: 1.1825 train_acc: 0.9929\n",
      "Epoch 049 train_loss: 1.1874 train_acc: 0.9929\n",
      "Epoch 050 train_loss: 1.1782 train_acc: 1.0000\n",
      "Epoch 051 train_loss: 1.1816 train_acc: 1.0000\n",
      "Epoch 052 train_loss: 1.1846 train_acc: 1.0000\n",
      "Epoch 053 train_loss: 1.1862 train_acc: 1.0000\n",
      "Epoch 054 train_loss: 1.1759 train_acc: 1.0000\n",
      "Epoch 055 train_loss: 1.1864 train_acc: 0.9929\n",
      "Epoch 056 train_loss: 1.1829 train_acc: 0.9929\n",
      "Epoch 057 train_loss: 1.1771 train_acc: 1.0000\n",
      "Epoch 058 train_loss: 1.1872 train_acc: 1.0000\n",
      "Epoch 059 train_loss: 1.1799 train_acc: 1.0000\n",
      "Epoch 060 train_loss: 1.1905 train_acc: 1.0000\n",
      "Epoch 061 train_loss: 1.1865 train_acc: 1.0000\n",
      "Epoch 062 train_loss: 1.1789 train_acc: 1.0000\n",
      "Epoch 063 train_loss: 1.1920 train_acc: 1.0000\n",
      "Epoch 064 train_loss: 1.1915 train_acc: 0.9786\n",
      "Epoch 065 train_loss: 1.1836 train_acc: 1.0000\n",
      "Epoch 066 train_loss: 1.1818 train_acc: 1.0000\n",
      "Epoch 067 train_loss: 1.1825 train_acc: 1.0000\n",
      "Epoch 068 train_loss: 1.1850 train_acc: 1.0000\n",
      "Epoch 069 train_loss: 1.1833 train_acc: 1.0000\n",
      "Epoch 070 train_loss: 1.1829 train_acc: 1.0000\n",
      "Epoch 071 train_loss: 1.1818 train_acc: 1.0000\n",
      "Epoch 072 train_loss: 1.2003 train_acc: 0.9857\n",
      "Epoch 073 train_loss: 1.1854 train_acc: 0.9929\n",
      "Epoch 074 train_loss: 1.1831 train_acc: 0.9929\n",
      "Epoch 075 train_loss: 1.1862 train_acc: 0.9929\n",
      "Epoch 076 train_loss: 1.1839 train_acc: 1.0000\n",
      "Epoch 077 train_loss: 1.1807 train_acc: 1.0000\n",
      "Epoch 078 train_loss: 1.1778 train_acc: 1.0000\n",
      "Epoch 079 train_loss: 1.1778 train_acc: 1.0000\n",
      "Epoch 080 train_loss: 1.1791 train_acc: 1.0000\n",
      "Epoch 081 train_loss: 1.1797 train_acc: 1.0000\n",
      "Epoch 082 train_loss: 1.1928 train_acc: 1.0000\n",
      "Epoch 083 train_loss: 1.1838 train_acc: 0.9929\n",
      "Epoch 084 train_loss: 1.1845 train_acc: 0.9929\n",
      "Epoch 085 train_loss: 1.1866 train_acc: 0.9929\n",
      "Epoch 086 train_loss: 1.1829 train_acc: 1.0000\n",
      "Epoch 087 train_loss: 1.1760 train_acc: 1.0000\n",
      "Epoch 088 train_loss: 1.2016 train_acc: 0.9929\n",
      "Epoch 089 train_loss: 1.1922 train_acc: 1.0000\n",
      "Epoch 090 train_loss: 1.1778 train_acc: 1.0000\n",
      "Epoch 091 train_loss: 1.1840 train_acc: 0.9929\n",
      "Epoch 092 train_loss: 1.1803 train_acc: 1.0000\n",
      "Epoch 093 train_loss: 1.1799 train_acc: 1.0000\n",
      "Epoch 094 train_loss: 1.1786 train_acc: 1.0000\n",
      "Epoch 095 train_loss: 1.1819 train_acc: 0.9929\n",
      "Epoch 096 train_loss: 1.1786 train_acc: 1.0000\n",
      "Epoch 097 train_loss: 1.1859 train_acc: 1.0000\n",
      "Epoch 098 train_loss: 1.1918 train_acc: 0.9929\n",
      "Epoch 099 train_loss: 1.1869 train_acc: 0.9929\n",
      "Epoch 100 train_loss: 1.1948 train_acc: 0.9857\n",
      "Epoch 101 train_loss: 1.1844 train_acc: 1.0000\n",
      "Epoch 102 train_loss: 1.1775 train_acc: 1.0000\n",
      "Epoch 103 train_loss: 1.1889 train_acc: 0.9929\n",
      "Epoch 104 train_loss: 1.1766 train_acc: 1.0000\n",
      "Epoch 105 train_loss: 1.1848 train_acc: 0.9929\n",
      "Epoch 106 train_loss: 1.1792 train_acc: 1.0000\n",
      "Epoch 107 train_loss: 1.1851 train_acc: 0.9929\n",
      "Epoch 108 train_loss: 1.1841 train_acc: 1.0000\n",
      "Epoch 109 train_loss: 1.1815 train_acc: 0.9929\n",
      "Epoch 110 train_loss: 1.1750 train_acc: 1.0000\n",
      "Epoch 111 train_loss: 1.1801 train_acc: 1.0000\n",
      "Epoch 112 train_loss: 1.1776 train_acc: 1.0000\n",
      "Epoch 113 train_loss: 1.1865 train_acc: 0.9929\n",
      "Epoch 114 train_loss: 1.1796 train_acc: 1.0000\n",
      "Epoch 115 train_loss: 1.1777 train_acc: 1.0000\n",
      "Epoch 116 train_loss: 1.1772 train_acc: 1.0000\n",
      "Epoch 117 train_loss: 1.1784 train_acc: 1.0000\n",
      "Epoch 118 train_loss: 1.1755 train_acc: 1.0000\n",
      "Epoch 119 train_loss: 1.1761 train_acc: 1.0000\n",
      "Epoch 120 train_loss: 1.1884 train_acc: 0.9857\n",
      "Epoch 121 train_loss: 1.1820 train_acc: 1.0000\n",
      "Epoch 122 train_loss: 1.1851 train_acc: 0.9929\n",
      "Epoch 123 train_loss: 1.1792 train_acc: 1.0000\n",
      "Epoch 124 train_loss: 1.1769 train_acc: 1.0000\n",
      "Epoch 125 train_loss: 1.1736 train_acc: 1.0000\n",
      "Epoch 126 train_loss: 1.1829 train_acc: 0.9929\n",
      "Epoch 127 train_loss: 1.1764 train_acc: 1.0000\n",
      "Epoch 128 train_loss: 1.1748 train_acc: 1.0000\n",
      "Epoch 129 train_loss: 1.1799 train_acc: 1.0000\n",
      "Epoch 130 train_loss: 1.1742 train_acc: 1.0000\n",
      "Epoch 131 train_loss: 1.1840 train_acc: 0.9857\n",
      "Epoch 132 train_loss: 1.1746 train_acc: 1.0000\n",
      "Epoch 133 train_loss: 1.1777 train_acc: 1.0000\n",
      "Epoch 134 train_loss: 1.1763 train_acc: 1.0000\n",
      "Epoch 135 train_loss: 1.1759 train_acc: 1.0000\n",
      "Epoch 136 train_loss: 1.1848 train_acc: 0.9929\n",
      "Epoch 137 train_loss: 1.1748 train_acc: 1.0000\n",
      "Epoch 138 train_loss: 1.1769 train_acc: 1.0000\n",
      "Epoch 139 train_loss: 1.1799 train_acc: 1.0000\n",
      "Epoch 140 train_loss: 1.1768 train_acc: 0.9929\n",
      "Epoch 141 train_loss: 1.1781 train_acc: 1.0000\n",
      "Epoch 142 train_loss: 1.1784 train_acc: 1.0000\n",
      "Epoch 143 train_loss: 1.1790 train_acc: 1.0000\n",
      "Epoch 144 train_loss: 1.1703 train_acc: 1.0000\n",
      "Epoch 145 train_loss: 1.1759 train_acc: 1.0000\n",
      "Epoch 146 train_loss: 1.1797 train_acc: 1.0000\n",
      "Epoch 147 train_loss: 1.1846 train_acc: 0.9857\n",
      "Epoch 148 train_loss: 1.1772 train_acc: 0.9929\n",
      "Epoch 149 train_loss: 1.1819 train_acc: 1.0000\n",
      "Epoch 150 train_loss: 1.1815 train_acc: 1.0000\n",
      "Epoch 151 train_loss: 1.1752 train_acc: 1.0000\n",
      "Epoch 152 train_loss: 1.1838 train_acc: 1.0000\n",
      "Epoch 153 train_loss: 1.1812 train_acc: 1.0000\n",
      "Epoch 154 train_loss: 1.1806 train_acc: 1.0000\n",
      "Epoch 155 train_loss: 1.1812 train_acc: 0.9929\n",
      "Epoch 156 train_loss: 1.1749 train_acc: 1.0000\n",
      "Epoch 157 train_loss: 1.1811 train_acc: 1.0000\n",
      "Epoch 158 train_loss: 1.1784 train_acc: 1.0000\n",
      "Epoch 159 train_loss: 1.1765 train_acc: 1.0000\n",
      "Epoch 160 train_loss: 1.1825 train_acc: 1.0000\n",
      "Epoch 161 train_loss: 1.1789 train_acc: 1.0000\n",
      "Epoch 162 train_loss: 1.1791 train_acc: 1.0000\n",
      "Epoch 163 train_loss: 1.1768 train_acc: 1.0000\n",
      "Epoch 164 train_loss: 1.1792 train_acc: 1.0000\n",
      "Epoch 165 train_loss: 1.1798 train_acc: 1.0000\n",
      "Epoch 166 train_loss: 1.1769 train_acc: 1.0000\n",
      "Epoch 167 train_loss: 1.1773 train_acc: 1.0000\n",
      "Epoch 168 train_loss: 1.1815 train_acc: 1.0000\n",
      "Epoch 169 train_loss: 1.1736 train_acc: 1.0000\n",
      "Epoch 170 train_loss: 1.1781 train_acc: 1.0000\n",
      "Epoch 171 train_loss: 1.1702 train_acc: 1.0000\n",
      "Epoch 172 train_loss: 1.1791 train_acc: 1.0000\n",
      "Epoch 173 train_loss: 1.1810 train_acc: 0.9929\n",
      "Epoch 174 train_loss: 1.1692 train_acc: 1.0000\n",
      "Epoch 175 train_loss: 1.1779 train_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176 train_loss: 1.1897 train_acc: 0.9786\n",
      "Epoch 177 train_loss: 1.1754 train_acc: 1.0000\n",
      "Epoch 178 train_loss: 1.1814 train_acc: 1.0000\n",
      "Epoch 179 train_loss: 1.1712 train_acc: 1.0000\n",
      "Epoch 180 train_loss: 1.1826 train_acc: 0.9929\n",
      "Epoch 181 train_loss: 1.1781 train_acc: 1.0000\n",
      "Epoch 182 train_loss: 1.1734 train_acc: 1.0000\n",
      "Epoch 183 train_loss: 1.1754 train_acc: 1.0000\n",
      "Epoch 184 train_loss: 1.1812 train_acc: 1.0000\n",
      "Epoch 185 train_loss: 1.1787 train_acc: 1.0000\n",
      "Epoch 186 train_loss: 1.1731 train_acc: 1.0000\n",
      "Epoch 187 train_loss: 1.1751 train_acc: 1.0000\n",
      "Epoch 188 train_loss: 1.1723 train_acc: 1.0000\n",
      "Epoch 189 train_loss: 1.1713 train_acc: 1.0000\n",
      "Epoch 190 train_loss: 1.1868 train_acc: 0.9857\n",
      "Epoch 191 train_loss: 1.1803 train_acc: 1.0000\n",
      "Epoch 192 train_loss: 1.1746 train_acc: 1.0000\n",
      "Epoch 193 train_loss: 1.1735 train_acc: 1.0000\n",
      "Epoch 194 train_loss: 1.1724 train_acc: 1.0000\n",
      "Epoch 195 train_loss: 1.1814 train_acc: 0.9929\n",
      "Epoch 196 train_loss: 1.1749 train_acc: 0.9929\n",
      "Epoch 197 train_loss: 1.1756 train_acc: 0.9929\n",
      "Epoch 198 train_loss: 1.1790 train_acc: 1.0000\n",
      "Epoch 199 train_loss: 1.1776 train_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(200):\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, pred = torch.max(out[data.train_mask], dim=1)\n",
    "    correct = (pred == data.y[data.train_mask]).sum().item()\n",
    "    acc = correct/data.train_mask.sum().item()\n",
    "\n",
    "    print('Epoch {:03d} train_loss: {:.4f} train_acc: {:.4f}'.format(\n",
    "        epoch, loss.item(), acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 1.3855 test_acc: 0.8040\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = model(data)\n",
    "loss = criterion(out[data.test_mask], data.y[data.test_mask])\n",
    "_, pred = torch.max(out[data.test_mask], dim=1)\n",
    "correct = (pred == data.y[data.test_mask]).sum().item()\n",
    "acc = correct/data.test_mask.sum().item()\n",
    "print(\"test_loss: {:.4f} test_acc: {:.4f}\".format(loss.item(), acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pyg",
   "language": "python",
   "name": "env_pyg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
